{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 综合评价算法\n",
    "\n",
    "## 引言：综合评价模型的基本概念\n",
    "\n",
    "### 评价模型简介\n",
    "\n",
    "评价模型是一种工具，用于**量化和评估不同的对象或情况**，它通常用于**决策支持、绩效评估、资源分配**等场景，这种模型的主要目的是将**主观或定性的评价**转化为**客观或定量的评分**。\n",
    "\n",
    "### 综合评价模型的组成要素\n",
    "\n",
    "1. **被评价对象与主体**：评价对象是评价的**焦点**，如一个项目、产品或服务。评价主体则是**进行评价的个人或团队**。\n",
    "   \n",
    "2. **评价标准与因素**：这些是用来评价对象的准则，涉及到评价对象的重要性和质量。因素可能是**宏观**的，如可行性、效益等。\n",
    "   \n",
    "3. **评价指标**：这些是**具体的、可量化的**标准，用于对评价对象进行**细致的评估**。它们应该遵循“四性原则”：可度量性、典型性、独立性和内涵性。\n",
    "   \n",
    "4. **评价方法**：这是评价指标的**具体应用方式**，将抽象的概念转化为量化的指标。\n",
    "   \n",
    "5. **综合评价模型**：这个模型整合了以上所有要素，形成一个完整的评价系统。\n",
    "\n",
    "### 综合评价算法实施步骤\n",
    "\n",
    "1. **建立模型**：根据具体问题定义评价模型的目标和对象。\n",
    "\n",
    "2. **预处理**：\n",
    "   - **归一化**：统一不同指标的量度标准，使其可比。\n",
    "   - **相关性检查**：确定指标之间的相关性，以避免重复或相互影响。\n",
    "\n",
    "3. **明确评价标准**：确定评价的主要准则和它们的重要性。\n",
    "\n",
    "4. **评价因素与指标细化**：按照可度量性（重要性）、典型性（不是越多越好）、独立性（可区分且独立）和内涵性（清晰的现实意义）原则，将评价因素细化为具体的评价指标。\n",
    "\n",
    "5. **构建评价体系**：将评价因素和指标组织成一个结构化的体系。\n",
    "\n",
    "6. **处理指标**：将评价指标转化为可量化的数值，如通过排序或计算逆序数等方法。\n",
    "\n",
    "### 评价模型的应用场景\n",
    "\n",
    "评价模型适用于**需要量化比较和评估**的情况，尤其是当**评价标准不完全客观或有多个标准**需要考虑时。\n",
    "\n",
    "#### 1. 货比三家\n",
    "- **场景**：比较不同产品或服务的优劣。\n",
    "- **适用性**：当需要对一个或多个维度（如价格、质量、功能）进行横向比较时，评价模型可以帮助量化这些标准，并提供一个结果或方案。\n",
    "\n",
    "#### 2. KPI（关键绩效指标）\n",
    "- **场景**：评估员工、部门或公司的绩效。\n",
    "- **适用性**：适用于将多个绩效指标综合考虑，特别是当这些指标各自重要性不同时，评价模型能帮助分配不同的权重。\n",
    "\n",
    "#### 3. 瘦死骆驼比马大\n",
    "- **场景**：比较两个在不同方面表现不一的对象。\n",
    "- **适用性**：当比较对象在某些方面优于另一个，而在其他方面则不如时，评价模型可以提供一个公平的比较方法，通过量化各个方面的重要性。\n",
    "\n",
    "#### 4. 考试类（考研、高考分值划分）\n",
    "- **场景**：学术或技能评估。\n",
    "- **适用性**：评价模型适用于将不同科目或评估标准合成一个总分，尤其在每个标准的重要性不同时。\n",
    "\n",
    "#### 5. 主观倾向性评价\n",
    "- **场景**：涉及专家意见、个人偏好的评估（如面试评分）。\n",
    "- **适用性**：评价模型可以帮助规范化这些主观判断，通过预先定义的标准和权重来减少个人偏见。\n",
    "\n",
    "### 评价指标的具体应用方法\n",
    "   \n",
    "- 求权重方法\n",
    "  1. AHP法（主观）\n",
    "  2. 熵权法（客观）\n",
    "  3. CRITIC法（客观）\n",
    "  4. 博弈论综合权重法（主客观结合）\n",
    "  5. 最小相对信息熵（主客观结合）\n",
    "   \n",
    "- 综合评价法\n",
    "  1. TOPSIS法\n",
    "  2. 秩和比法\n",
    "  3. VIKOR法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 熵权-秩和比法（AHP模型、熵权法、秩和比法）\n",
    "\n",
    "### AHP模型（Analytic Hierarchy Process）(主观因素较大)\n",
    "\n",
    "层次分析法（AHP，Analytic Hierarchy Process）是一个非常强大的决策支持工具，广泛应用于各种复杂决策问题中。AHP由美国运筹学家托马斯·L·萨蒂（Thomas L. Saaty）在20世纪70年代提出，其主要特点是通过将复杂的决策问题分解为易于理解和分析的层次结构。\n",
    "\n",
    "#### AHP模型的基本步骤\n",
    "\n",
    "1. **建立层次结构**：\n",
    "   - **目标层**：决策的主要目的或需要解决的问题。\n",
    "   - **准则层**：影响决策的因素或标准。\n",
    "   - **方案层**：可供选择的备选方案。\n",
    "\n",
    "   例如，如果一个公司要选择新的办公地点，目标层是选择最佳的办公地点，准则层可能包括成本、位置、交通便利性等，方案层则是具体的地点选项。\n",
    "\n",
    "2. **构造判断矩阵**：\n",
    "   - 在每一个准则层，通过成对比较各准则的重要性，构造出一个判断矩阵。\n",
    "   - 利用1到9的标度进行评分，其中1表示两个元素同等重要，9表示一个元素比另一个元素重要得多。\n",
    "   - 这个过程主要依赖专家的主观判断。\n",
    "\n",
    "3. **一致性检验**：\n",
    "   - 为了确保判断的合理性，需要对判断矩阵进行一致性检验。\n",
    "   - 通过计算一致性比例（CR，Consistency Ratio）来评估这个矩阵的一致性。一般来说，CR值小于0.1时，判断矩阵的一致性是可以接受的。\n",
    "\n",
    "4. **合成权重的计算**：\n",
    "   - 从判断矩阵中计算出各准则或方案的相对权重。\n",
    "   - 这些权重反映了各准则或方案相对于总目标的重要性。\n",
    "\n",
    "5. **选择最佳方案**：\n",
    "   - 结合各层的权重，计算出各个方案的综合评分。\n",
    "   - 最高分的方案被认为是最佳选择。\n",
    "\n",
    "#### AHP模型的应用\n",
    "AHP模型特别适用于那些涉及多个准则或标准的决策问题。它的优势在于能够结合定性和定量的分析，使决策过程更加透明和系统化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据的归一化\n",
    "\n",
    "实际上，在构建AHP模型时，首先要对数据进行归一化，下面是三种常用的归一化方法：\n",
    "\n",
    "1. Min-max normalization (Rescaling)\n",
    "2. Mean normalization （与方法1相似，只不过使数据以0为均值）\n",
    "3. Z-score normalization (Standardization)\n",
    "4. L2 normalization\n",
    "\n",
    "对于其中三种较重要的方法的对比\n",
    "\n",
    "| 归一化方法 | 适用场景 | 说明 |\n",
    "|------------|----------|------|\n",
    "| Min-max normalization (Rescaling) |  当数据需要被限制在特定范围内（如0到1）<br> 神经网络等需要固定范围输入的算法<br> 数据特征的原始范围差异较大 | 适用于需要将数据规范到固定区间，特别是在需要保留原始数据中的最小和最大值信息时 |\n",
    "| Z-score normalization (Standardization) |  数据假定接近正态分布<br> 线性回归、逻辑回归、判别分析等统计方法<br> 当数据分布不明显偏斜（没有极端值） | 当算法假设数据是正态分布，或者需要消除量纲影响、标准化数据特征的时候适用 |\n",
    "| L2 normalization (Normalization) |  需要保持数据点间的相对距离<br> K近邻算法、聚类算法<br> 特征尺度差异对分析结果影响显著的情况（如文本数据） | 在非参数模型中，特别是需要保持数据点方向的情况下适用，适合特征间相对比例重要的场景 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.353 0.744 0.59  ... 0.501 0.234 0.483]\n",
      " [0.059 0.427 0.541 ... 0.396 0.117 0.167]\n",
      " [0.471 0.92  0.525 ... 0.347 0.254 0.183]\n",
      " ...\n",
      " [0.294 0.608 0.59  ... 0.39  0.071 0.15 ]\n",
      " [0.059 0.633 0.492 ... 0.449 0.116 0.433]\n",
      " [0.059 0.467 0.574 ... 0.453 0.101 0.033]]\n"
     ]
    }
   ],
   "source": [
    "# Min-max normalization (Rescaling) (按比例缩放，一般不推荐)\n",
    "# K近邻算法\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import set_printoptions\n",
    "from pandas import read_csv\n",
    "\n",
    "filename = 'pima_data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = read_csv(filename, names = names)\n",
    "\n",
    "array = data.values\n",
    "X = array[ : , 0 : 8]\n",
    "Y = array[ : , 8]\n",
    "transformer = MinMaxScaler(feature_range = (0, 1)) # feature_range: 指定缩放范围\n",
    "newX = transformer.fit_transform(X) # fit_transform: 拟合数据，然后转化它将其转化为标准形式\n",
    "set_printoptions(precision = 3)\n",
    "\n",
    "print(newX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64   0.848  0.15  ...  0.204  0.468  1.426]\n",
      " [-0.845 -1.123 -0.161 ... -0.684 -0.365 -0.191]\n",
      " [ 1.234  1.944 -0.264 ... -1.103  0.604 -0.106]\n",
      " ...\n",
      " [ 0.343  0.003  0.15  ... -0.735 -0.685 -0.276]\n",
      " [-0.845  0.16  -0.471 ... -0.24  -0.371  1.171]\n",
      " [-0.845 -0.873  0.046 ... -0.202 -0.474 -0.871]]\n"
     ]
    }
   ],
   "source": [
    "# Z-score normalization (Standardization)（数据假定接近正态分布，使数据具有0均值和单位方差）\n",
    "# 线性回归、逻辑回归、判别分析\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "transformer = StandardScaler().fit(X)\n",
    "newX = transformer.transform(X)\n",
    "\n",
    "print(newX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.034 0.828 0.403 ... 0.188 0.004 0.28 ]\n",
      " [0.008 0.716 0.556 ... 0.224 0.003 0.261]\n",
      " [0.04  0.924 0.323 ... 0.118 0.003 0.162]\n",
      " ...\n",
      " [0.027 0.651 0.388 ... 0.141 0.001 0.161]\n",
      " [0.007 0.838 0.399 ... 0.2   0.002 0.313]\n",
      " [0.008 0.736 0.554 ... 0.241 0.002 0.182]]\n"
     ]
    }
   ],
   "source": [
    "# L2 normalization (Normalization)（调整数据集中每个样本的特征向量，使其具有单位范数（长度为1），这会使得每个样本的各个特征值的平方和为1）（更推荐）\n",
    "# 神经网络、文本分类、聚类算法、K近邻算法\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "transformer = Normalizer().fit(X)\n",
    "newX = transformer.transform(X)\n",
    "\n",
    "print(newX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 判断矩阵\n",
    "\n",
    "![Judgement Matrix](Judgement_Matrix.jpg)\n",
    "\n",
    "- 算数平均法求权重\n",
    "  1. 将判断矩阵按照归一化（每个元素除以所在列的和）\n",
    "  2. 将归一化的各列相加（即为按行求和）\n",
    "  3. 将相加后得到向量每个元素除以特征值，可以得到权重向量\n",
    "\n",
    "- 一致性检验\n",
    "  1. 为了防止出现矛盾，则判断矩阵满足 $ a_{i,k} \\cdot a_{k,j} = a_{i,j} $，这样的矩阵为一致矩阵。（穷举法）\n",
    "  2. 计算一致性指标CI，查找对应的平均随机一致性指标RI，最终得到一致性比例系数$ CR = \\frac{CI}{RI} $，如果CR<0.1，则判断矩阵满足一致性\n",
    "\n",
    "$$ CI = \\frac{\\lambda_{max} - n}{n - 1} $$\n",
    "\n",
    "![RI](RI.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Vector: [0.1  0.15 0.2  0.25 0.3 ]\n",
      "Lambda Max (Maximum Eigenvalue): 1.3500\n",
      "Consistency Index (CI): -0.9125\n",
      "Random Index (RI) for n=5: 1.1200\n",
      "Consistency Ratio (CR): -0.8147\n",
      "Final Scores for Candidate Destinations: [7.15 6.65 6.9 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creating the judgment matrix\n",
    "x = 2\n",
    "judgment_matrix = np.array([\n",
    "    [1, x, x, x, x],\n",
    "    [1/x, 1, x, x, x],\n",
    "    [1/x, 1/x, 1, x, x],\n",
    "    [1/x, 1/x, 1/x, 1, x],\n",
    "    [1/x, 1/x, 1/x, 1/x, 1]\n",
    "])\n",
    "\n",
    "# Calculating the column sum and weight vector\n",
    "column_sum = np.sum(judgment_matrix, axis=0)\n",
    "weight_vector = column_sum / np.sum(column_sum)\n",
    "\n",
    "# Calculating the consistency index and ratio\n",
    "n = len(weight_vector)\n",
    "random_index = [0, 0, 0.52, 0.89, 1.12, 1.26, 1.36, 1.41, 1.46, 1.49]\n",
    "lambda_max = np.dot(weight_vector, column_sum) / n\n",
    "consistency_index = (lambda_max - n) / (n - 1)\n",
    "consistency_ratio = consistency_index / random_index[n - 1]\n",
    "\n",
    "# Normalizing the weight vector\n",
    "normalized_weight_vector = weight_vector / np.sum(weight_vector)\n",
    "\n",
    "# Evaluating and scoring candidate destinations\n",
    "candidate_destinations = [\"Destination 1\", \"Destination 2\", \"Destination 3\"]\n",
    "evaluation_scores = np.array([\n",
    "    [8, 5, 7, 6, 9],\n",
    "    [9, 4, 8, 7, 6],\n",
    "    [7, 6, 9, 8, 5]\n",
    "])\n",
    "\n",
    "# Calculating the final scores\n",
    "final_scores = np.dot(normalized_weight_vector, evaluation_scores.T)\n",
    "\n",
    "# Formatted print statements for each result\n",
    "print(\"Weight Vector:\", weight_vector)\n",
    "print(\"Lambda Max (Maximum Eigenvalue): {:.4f}\".format(lambda_max))\n",
    "print(\"Consistency Index (CI): {:.4f}\".format(consistency_index))\n",
    "print(\"Random Index (RI) for n=5: {:.4f}\".format(random_index[n - 1]))\n",
    "print(\"Consistency Ratio (CR): {:.4f}\".format(consistency_ratio))\n",
    "print(\"Final Scores for Candidate Destinations:\", final_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 熵权法（客观定权）\n",
    "\n",
    "#### 熵权法相关基本概念\n",
    "\n",
    "1. **信息论的基础**：由香农提出的，它用数学的方法来量化信息。在信息论中，**信息熵**是一个核心概念，它描述了一个系统的不确定性。在这里，我们用它来描述一个指标的不确定性，也就是它的变化程度。信息熵越高，表示该指标的数据越分散，反之则数据越集中。\n",
    "\n",
    "2. **信息熵（Information Entropy）**：这个概念最初由克劳德·香农在信息论中引入，用以衡量**信息的不确定性或无序程度**。比如如果预报员说“明天下雨的概率是50%”，这包含了很多不确定性。但如果他们说“明天一定下雨”，这就很确定，不确定性很小。**信息熵正是用来量化这种不确定性的。**\n",
    "\n",
    "3. **熵权法（Entropy Weight Method）**：这是一种基于信息熵原理的数据分析方法。在评价多个指标时，我们需要决定每个指标的重要性或权重。熵权法通过计算每个指标的信息熵来决定其权重，如果一个指标的**数据变化很大（即离散程度高）**，它的信息熵就会**较低**，意味着这个指标提供了更多的信息，因此应该赋予**更大的权重**。\n",
    "\n",
    "简单来说，就像你在一个有很多声音的房间里，那些声音变化最大、最不可预测的（即“最吵闹的”），往往最能吸引你的注意。在熵权法中，这些“最吵闹的”指标，因为它们的信息量大，所以在整体评价中占据更重要的地位。\n",
    "\n",
    "\n",
    "#### 熵权法的计算步骤\n",
    "\n",
    "1. 标准化\n",
    "   - **目的**：将所有指标转换为非负数值，以便它们可以被解释为概率。\n",
    "   - **方法**：使用最小-最大标准化（Min-max normalization），对于矩阵中的每个元素 $ x $，标准化公式为：\n",
    "   $$ x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} $$\n",
    "   - **结果**：得到一个新的矩阵，所有元素都在 [0,1] 区间。\n",
    "\n",
    "2. 计算比重（概率）\n",
    "   - **目的**：确定每个样本在每个指标下的相对重要性。\n",
    "   - **公式**：对于第 $ i $ 个样本的第 $ j $ 个指标 $ x_{ij} $，其比重 $ p_{ij} $ 计算为：\n",
    "   $$ p_{ij} = \\frac{x'_{ij}}{\\sum_{i=1}^{n} x'_{ij}} $$\n",
    "   - **说明**：这里 $ x'_{ij} $ 是标准化后的值，分母是第 $ j $ 个指标下所有样本标准化值的总和。\n",
    "\n",
    "3. 计算信息熵\n",
    "   - **目的**：评估每个指标的不确定性。\n",
    "   - **公式**：第 $ j $ 个指标的信息熵 $ e_j $ 由下式给出：\n",
    "   $$ e_j = -\\frac{1}{\\ln(n)} \\sum_{i=1}^{n} p_{ij} \\ln(p_{ij}) $$\n",
    "   - **说明**：这里 $ n $ 是样本的数量，分母 $ \\ln(n) $ 是为了正规化信息熵值。\n",
    "\n",
    "4. 计算差异值\n",
    "   - **公式**：第 $ j $ 个指标的差异值 $ d_j $ 计算为：\n",
    "   $$ d_j = 1 - e_j $$\n",
    "\n",
    "5. 计算权重\n",
    "   - **目的**：确定每个指标在综合评价中的相对重要性。\n",
    "   - **公式**：第 $ j $ 个指标的权重 $ \\omega_j $ 计算为：\n",
    "   $$ \\omega_j = \\frac{d_j}{\\sum_{j=1}^{m} d_j} $$\n",
    "   - **说明**：权重是各个指标的差异值归一化的结果，确保所有权重之和为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    信息熵值  信息效用值     权重\n",
      "0 0.9202 0.1805 0.1805\n",
      "1 0.9049 0.2152 0.2152\n",
      "2 0.9333 0.1508 0.1508\n",
      "3 0.9282 0.1623 0.1623\n",
      "4 0.8713 0.2912 0.2912\n",
      "综合得分: ['0.5986', '0.6565', '0.5377', '0.5502', '0.6206', '0.5151', '0.3960', '0.7467', '0.4971', '0.3345']\n"
     ]
    }
   ],
   "source": [
    "# 熵权法\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 使用随机数生成一个示例数据\n",
    "np.random.seed(0)\n",
    "data = np.random.rand(10, 5)\n",
    "\n",
    "# 归一化数据\n",
    "normalized_data = (data - data.min(axis = 0)) / (data.max(axis = 0) - data.min(axis = 0))\n",
    "\n",
    "# 计算比重（概率）\n",
    "column_sums = np.sum(normalized_data, axis = 0)\n",
    "probability_matrix = normalized_data / column_sums\n",
    "\n",
    "# 为了避免对零取对数，给概率矩阵中的零值加上一个非常小的正数\n",
    "epsilon = 1e-10\n",
    "probability_matrix = np.where(probability_matrix == 0, epsilon, probability_matrix)\n",
    "\n",
    "# 计算信息熵值\n",
    "entropy_values = -np.sum(probability_matrix * np.log(probability_matrix), axis=0) / np.log(len(data))\n",
    "\n",
    "# 计算信息效用值\n",
    "utility_values = 1 - entropy_values\n",
    "utility_values = utility_values / np.sum(utility_values)\n",
    "\n",
    "# 计算权重\n",
    "weights = utility_values / np.sum(utility_values)\n",
    "\n",
    "# 设置显示格式，使得浮点数保留四位小数\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "# 重新创建表格以应用新的显示格式\n",
    "display_table = pd.DataFrame({\n",
    "    '信息熵值': entropy_values,\n",
    "    '信息效用值': utility_values,\n",
    "    '权重': weights\n",
    "})\n",
    "\n",
    "# 计算综合得分\n",
    "composite_scores = np.sum(normalized_data * weights, axis = 1)\n",
    "\n",
    "# 输出结果\n",
    "print(display_table)\n",
    "print(\"综合得分:\", ['{:.4f}'.format(score) for score in composite_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRITIC法\n",
    "\n",
    "CRITIC法（Criteria Importance Through Intercriteria Correlation）是一种客观赋权方法，用于多标准决策分析。它由Diakoulaki等人在1995年提出，主要通过考察标准间的冲突和标准自身的对比强度来确定权重。\n",
    "\n",
    "#### CRITIC方法的基本步骤\n",
    "\n",
    "1. **数据标准化**：\n",
    "   - 将原始数据转化为无量纲的形式，这通常通过范围归一化来实现，使得数据在[0, 1]区间内。\n",
    "\n",
    "2. **计算标准的标准差（或方差）**：\n",
    "   - 标准差（或方差）衡量了每个标准的**变异程度**。变异程度大意味着标准对比强度高，因此应赋予更大的权重。\n",
    "\n",
    "3. **计算标准间的冲突**：\n",
    "   - 冲突指标准间的**相关性**。标准间相关性低（冲突高）的标准包含更多的独立信息，因此应赋予更大的权重。\n",
    "\n",
    "4. **确定权重**：\n",
    "   - 综合标准的对比强度和冲突来确定权重。具体地，权重计算为每个标准的方差加上与其他标准的最大冲突，再减去该标准与其他所有标准的冲突总和，然后归一化处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重: ['0.2145', '0.1920', '0.2064', '0.2079', '0.1792']\n"
     ]
    }
   ],
   "source": [
    "# CRITIC法\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 使用随机数生成一个示例数据\n",
    "np.random.seed(0)\n",
    "data = np.random.rand(10, 5)\n",
    "\n",
    "# 归一化数据\n",
    "normalized_data = (data - data.min(axis = 0)) / (data.max(axis = 0) - data.min(axis = 0))\n",
    "\n",
    "# 计算方差\n",
    "variance = np.var(normalized_data, axis=0)\n",
    "\n",
    "# 计算标准间的冲突\n",
    "conflict = np.zeros((normalized_data.shape[1], normalized_data.shape[1]))\n",
    "for i in range(normalized_data.shape[1]):\n",
    "    for j in range(normalized_data.shape[1]):\n",
    "        if i != j:\n",
    "            # 计算第i个和第j个标准之间的差异性\n",
    "            conflict[i, j] = np.sum(np.abs(normalized_data[:, i] - normalized_data[:, j]))\n",
    "\n",
    "# 计算信息量\n",
    "information = 1 / (variance + np.max(conflict, axis = 1) - np.sum(conflict, axis = 1))\n",
    "\n",
    "# 计算权重\n",
    "weights = information / np.sum(information)\n",
    "\n",
    "# 输出权重\n",
    "print(\"权重:\", ['{:.4f}'.format(weight) for weight in weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 秩和比法\n",
    "\n",
    "#### 秩和比法的基本原理\n",
    "\n",
    "秩和比法（Rank Sum Ratio, RSR）是一种用于评价和比较多个对象或实验条件下的综合效果的非参数统计方法。这种方法不依赖于数据的**具体分布**，而是通过数据的**相对排名**来进行分析。\n",
    "\n",
    "在这个方法中，我们通常有一个 $ n $ 行 $ m $ 列的数据矩阵，其中 $ n $ 是样本的数量，$ m $ 是评价指标的数量。这个矩阵可以看作是一个比赛的成绩单，每一行代表一个参赛者（或评价对象），每一列代表一个比赛项目的成绩。\n",
    "\n",
    "基本原理可以这样理解：\n",
    "\n",
    "1. **秩转换**：我们首先对每个指标（列）的数据进行排序，并赋予每个数据一个秩次。例如，在赛跑比赛中，第一个跑过终点线的人获得第一名，也就是秩为1，以此类推。\n",
    "\n",
    "2. **无量纲化**：通过转换为秩，我们把原始数据转化为了无量纲的统计量，使得不同尺度或单位的数据可以公平比较。\n",
    "\n",
    "3. **RSR计算**：对于每个样本，我们将其所有指标的秩次相加得到一个总秩和，然后用这个总秩和除以一个标准化值（通常是该样本在所有指标上都获得最高秩时的秩和），得到秩和比。\n",
    "\n",
    "4. **综合评价**：通过RSR值，我们可以对样本进行排序，RSR值越大，表示样本在所有指标上的综合表现越好。\n",
    "\n",
    "#### 样本秩的概念：\n",
    "\n",
    "样本秩是指在一个样本集中，每个数值按照其大小顺序被赋予的排名。如果你有一组数值 $ x_1, x_2, \\ldots, x_n $，它们可以按照从小到大(或从大到小)排序。每个数值在排序后的位置就是它的秩。秩次为样本提供了一种在群体中的相对位置度量。\n",
    "\n",
    "例如，假设我们有一组数据 $ [3, 1, 4] $。当我们将它们排序 $ [1, 3, 4] $ 后，数值1的秩是1，数值3的秩是2，数值4的秩是3。\n",
    "\n",
    "样本秩处理的一个特殊情况是**并列秩**。如果数据中有相同的数值（即平局），我们会赋予它们的秩是它们在没有平局情况下应有秩次的平均值。\n",
    "\n",
    "#### 秩和比法基本步骤\n",
    "\n",
    "1. 选择评价指标并确定指标类型\n",
    "\n",
    "    在应用秩和比法之前，首先需要确定评价对象和评价指标。\n",
    "\n",
    "   - **效益型指标**：这类指标是“越大越好”的。\n",
    "   - **成本型指标**：这类指标是“越小越好”的。\n",
    "   - **中性型指标**：这类指标并不明确指出数值大或小更好，可能根据具体情况而定。\n",
    "\n",
    "    评价的第一步是构建一个数据矩阵，其中包括 $ n $ 个评价对象和 $ m $ 个评价指标。\n",
    "\n",
    "2. 秩和比法的两种概念\n",
    "\n",
    "   - **整次秩和比法**：这种方法要求按指标类型将每个指标下的评价对象值进行排序，并赋予秩次。对于效益型指标，值从小到大排列；对于成本型指标，值从大到小排列。如果存在相同的数值，即并列情况，赋予相同数值的平均秩。\n",
    "\n",
    "   - **非整次秩和比法**：这种方法旨在保持原始指标值的定量信息，编排的秩次与原始指标值之间存在定量的线性关系，从而避免在秩次化过程中损失信息。\n",
    "\n",
    "3. RSR计算方法\n",
    "\n",
    "   - **计算RSR（权重相同）**：\n",
    "    对于每个评价对象，计算其所有指标的秩次之和，并除以 $ n \\times m $，即评价对象数乘以指标数。\n",
    "        $$ RSR_i = \\frac{1}{n \\times m} \\sum_{j=1}^{m} R_{ij} $$\n",
    "        其中 $ R_{ij} $ 是第 $ i $ 个评价对象在第 $ j $ 个指标的秩次。\n",
    "\n",
    "   - **计算WRSR（权重不同）(更推荐）**：\n",
    "    对于每个评价对象，按照各指标的重要性赋予不同的权重 $ W_j $，计算加权秩和比。（ $ W_j $ 这个数据就是通过AHP或熵权法得到的）\n",
    "        $$ WRSR_i = \\frac{1}{n} \\sum_{j=1}^{m} W_j R_{ij} $$\n",
    "        其中 $ W_j $ 是第 $ j $ 个指标的权重。\n",
    "\n",
    "4. RSR分布的确认\n",
    "\n",
    "   - 通过计算所有评价对象的RSR或WRSR值，我们可以编制一个RSR（或WRSR）频率分布表。\n",
    "   - 列出各组频数 $ f $，计算各组累积频数 $ cf $，然后计算累积频率 $ p_i = \\frac{cf_i}{n} $。\n",
    "   - 将累积频率 $ p_i $ 转换为概率单位 $ probit_i $，即标准正态分布的 $ p_i $ 分位数加5。\n",
    "\n",
    "5. 计算直线回归方程\n",
    "\n",
    "   - 以累积频率对应的概率单位 $ probit_i $ 为自变量，以RSR（或WRSR）值为因变量，计算直线回归方程：\n",
    "  $$ RSR \\text{ (或 } WRSR \\text{) } = a + b \\times \\text{Probit} $$\n",
    "\n",
    "6. 分档排序\n",
    "\n",
    "   - 使用回归方程将评价对象进行分档排序，这可以基于它们的RSR或WRSR值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主客观评价的综合\n",
    "\n",
    "#### 博弈论综合权重法\n",
    "\n",
    "在决策分析中，权重综合是一种结合多个权重评价标准的方法，旨在综合不同来源或不同方法得出的权重，以反映每个评价标准的相对重要性。权重可以来源于主观评价（如AHP法）和客观算法（如熵权法、CRITIC法等），基于博弈论的权重综合法是一种旨在平衡主观与客观权重的方法。\n",
    "\n",
    "##### 博弈论综合权重法的基本思想：\n",
    "\n",
    "1. **主观权重**：通常由决策者或专家根据经验和直觉给出，反映了决策者的偏好和价值判断。\n",
    "\n",
    "2. **客观权重**：通过数据驱动的方法计算得出，试图通过数据的内在关系来反映各标准的重要性，不受个人偏好影响。\n",
    "\n",
    "3. **权重的一致性**：博弈论的目的是找到一个权重方案，使得主观权重和客观权重之间的差异最小。这相当于在决策者的偏好和数据的实际表现之间找到一个平衡点。\n",
    "\n",
    "4. **博弈论模型**：在此模型中，主观权重和客观权重被看作是参与博弈的两方，每方都试图推动最终的权重方案更接近于自己的权重。权重综合的过程可以被看作是一个协商过程，最终达到一个妥协解。\n",
    "\n",
    "##### 博弈论综合权重法的实现步骤：\n",
    "\n",
    "1. **确定主观权重和客观权重**：首先需要确定每个标准的主观权重和客观权重。\n",
    "\n",
    "2. **构建博弈矩阵**：构建一个矩阵或模型，其中包含主观权重和客观权重，以及它们之间的差异。\n",
    "\n",
    "3. **求解博弈**：通过数学或统计方法寻找一个解，使得主观权重与客观权重的差异最小。这通常涉及优化问题的求解。\n",
    "\n",
    "4. **得到综合权重**：解博弈得到的权重就是综合权重，它既考虑了专家的意见，也考虑了数据的客观信息。\n",
    "\n",
    "在实际操作中，综合权重的计算可能需要使用优化算法，如线性规划、非线性规划等，以确定使主客观权重离差最小的权重组合。这种方法适用于那些既需要考虑决策者的主观判断，又要兼顾客观数据分析结果的决策场景。通过博弈论综合权重法，可以确保最终的决策更加全面和均衡。\n",
    "\n",
    "![Game Theory Comprehensive Weight Method](Game%20Theory%20Comprehensive%20Weight%20Method.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最小相对信息熵\n",
    "\n",
    "最小相对信息熵（Minimum Relative Information Entropy）是一种综合权重的方法，它的目的是最小化主观权重和客观权重之间的相对离差。这种方法基于信息熵的概念，信息熵在这里用于衡量权重分布的不确定性。最小化相对信息熵可以被视为是寻找一个平衡点，使得两套权重尽可能地接近，同时保留它们各自的特点。\n",
    "\n",
    "##### 最小相对信息熵的数学描述：\n",
    "\n",
    "给定一组主观权重 $ \\alpha_j $ 和一组客观权重 $ \\beta_j $，目标是找到一组新的权重 $ w_j $，满足以下条件：\n",
    "\n",
    "1. 权重之和为1（即 $ \\sum_{j=1}^{n} w_j = 1 $），这是权重的基本要求。\n",
    "\n",
    "2. 每个权重都是非负的（即 $ w_j \\geq 0 $）。\n",
    "\n",
    "3. 最小化权重与主观权重、客观权重对数值的相对离差的加权和，这可以通过以下优化模型实现：\n",
    "\n",
    "   $$\n",
    "   \\text{minimize} \\quad \\sum_{j=1}^{n} w_j (\\ln w_j - \\ln \\alpha_j) + \\sum_{j=1}^{n} w_j (\\ln w_j - \\ln \\beta_j)\n",
    "   $$\n",
    "\n",
    "   上述表达式的最小化可以通过拉格朗日乘数法解决。\n",
    "\n",
    "##### 拉格朗日乘数法解决优化问题：\n",
    "\n",
    "对于上述的优化问题，拉格朗日乘数法引入一个额外的乘数（拉格朗日乘数），将约束条件整合到目标函数中，从而将约束优化问题转换为无约束问题。求解该问题后，可以得到最优权重。\n",
    "\n",
    "##### 最终权重的计算公式：\n",
    "\n",
    "根据上述优化问题的解决方案，综合权重 $ w_j $ 可以用以下公式计算：\n",
    "\n",
    "$$\n",
    "w_j = \\frac{(\\gamma_j \\alpha_j)^{0.5}}{\\sum_{j=1}^{n} (\\gamma_j \\alpha_j)^{0.5}}\n",
    "$$\n",
    "\n",
    "其中，$ \\gamma_j $ 是与客观权重 $ \\beta_j $ 相关的系数，这个系数的确定方法通常依赖于具体的问题和数据。\n",
    "\n",
    "##### 最小相对信息熵的应用：\n",
    "\n",
    "最小相对信息熵方法适用于需要融合主观评价和客观数据的情境，常见于复杂决策分析、风险评估、资源分配等领域。通过最小化主客观权重的离差，这种方法试图找到一个双方都能接受的共识权重，从而使得决策过程更为合理和公正。\n",
    "\n",
    "![Minimum Relative Information Entropy](Minimum%20Relative%20Information%20Entropy.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
