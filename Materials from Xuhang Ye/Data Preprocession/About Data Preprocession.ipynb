{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "\n",
    "## 为什么需要数据预处理\n",
    "\n",
    "1. **确保数据的质量**：在真实世界中，数据往往是**不完整（缺少某些感兴趣的属性值）**、**不一致（包含代码或者名称的差异）**并且**充满噪声（错误或异常值）**。\n",
    "\n",
    "2. **增强模型的性能**：高质量的数据可以显著提高分析模型的准确度和效率。\n",
    "\n",
    "3. **提高结果的可解释性**：干净且一致的数据使得分析结果更易于理解和解释。\n",
    "\n",
    "## 数据预处理的流程\n",
    "\n",
    "1. **导入数据**：首先，我们需要从各种来源导入数据。\n",
    "\n",
    "2. **整理数据**：\n",
    "   - **处理缺失值**：识别并处理数据中的缺失值。\n",
    "   - **识别和处理异常值**：对于数据中的不良部分，需要被剔除或修正。\n",
    "   - **数据清洗**：移除重复数据，纠正错误，确保数据干净可用。\n",
    "\n",
    "3. **格式化输入数据**：\n",
    "   - **数据转换**：根据需要将数据转换成适当的格式或结构。\n",
    "   - **标准化或归一化数据**：确保数据在同一尺度上，以便于分析。\n",
    "\n",
    "4. **总结数据**：\n",
    "    - 涉及到对数据的整体了解，包括它的分布、趋势和关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么需要处理数据特征\n",
    "\n",
    "1. **数据的重要性**：数据和特征决定了机器学习的上限，而模型和算法仅仅是尝试接近这个上限。好的特征能够显著提升模型性能。\n",
    "\n",
    "2. **特征工程的目的**：特征工程的核心是从原始数据中提取最合适的特征，供算法和模型使用。通过选择和转换输入的数据，可以增强模型的预测能力。\n",
    "\n",
    "3. **算法性能提升**：合适的特征选定可以减少模型复杂度，提高算法的准确性，并减少训练时间。\n",
    "\n",
    "## 特征选定的方法\n",
    "\n",
    "Scikit-learn 提供了多种特征处理方法，包括但不限于：\n",
    "\n",
    "1. **单变量特征选定**：这种方法考察单个变量与目标变量之间的关系。基于某种统计测试（如卡方测试、ANOVA），它选择与输出最相关的特征。\n",
    "\n",
    "2. **递归特征消除（Recursive Feature Elimination, RFE）**：这种方法通过递归减少特征的数量。它利用模型的权重来识别并去除最不重要的特征，逐步优化特征子集。\n",
    "\n",
    "3. **主成分分析（PCA）**：PCA 是一种降维技术，通过线性变换将数据转换为一组相互正交的变量（主成分），这些主成分捕捉了数据中的大部分变异。\n",
    "\n",
    "4. **特征的重要性**：一些模型（如决策树和随机森林）可以提供关于特征重要性的内置方法，帮助识别对模型预测最有贡献的特征。\n",
    "\n",
    "## 特征选定的优点\n",
    "\n",
    "1. **降低拟合度（Overfitting）**：去除不相关的特征可以减少模型过度拟合数据的风险，提高其泛化能力。\n",
    "\n",
    "2. **提高算法精度**：减少噪声数据，专注于对结果影响最大的特征，可以显著提高算法的预测准确度。\n",
    "\n",
    "3. **减少训练时间**：特征较少的数据集通常意味着更快的训练速度和更低的计算成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单变量特征选定\n",
    "\n",
    "### 单变量特征选定的基本概念\n",
    "\n",
    "当我们谈论单变量特征选择时，我们的目的是从**一组数据特征**中挑选出**最重要**的一部分。在数据科学中，就是使用各种统计方法，来评估每个特征和我们感兴趣的结果之间的相关性。\n",
    "\n",
    "1. **统计分析**：\n",
    "   - 我们使用统计方法来测试数据特征的“重要度”，这种测试告诉我们每个特征与结果之间关联的强度。\n",
    "\n",
    "2. **Skikit-learn 的 SelectKBest 类**：\n",
    "   - 这就像一个工具，帮助我们挑选出“最好的”特征。我们可以告诉这个工具我们想要选择多少个特征（比如最好的10个），它会基于统计测试的结果为你挑选出来。\n",
    "\n",
    "3. **卡方检验**：\n",
    "   - 卡方检验是一种特殊的统计测试。在数据科学中，卡方检验用来检测分类变量之间的关系。比如说，判断性别（男/女）是否影响某人喜欢苹果还是香蕉。\n",
    "\n",
    "### 单变量特征选定的应用场景\n",
    "\n",
    "假设你有一个包含多个特征的数据集，比如顾客的年龄、性别、收入等，你想预测他们是否会购买某产品。你可以使用单变量特征选择来找出哪些特征（比如年龄或收入）与购买决策最相关。通过卡方检验等方法，你可以鉴别出对于预测结果最有用的特征。\n",
    "\n",
    "### 单变量特征选定的操作步骤\n",
    "\n",
    "1. **准备数据**：收集数据，并确定预测的目标（比如顾客的购买行为）。\n",
    "\n",
    "2. **选择统计方法**：确定使用哪种统计方法来测试特征的重要性。如果特征和目标都是分类的（比如男/女，买/不买），卡方检验是个好选择。\n",
    "\n",
    "3. **应用 SelectKBest**：使用 Skikit-learn 的 SelectKBest 类，输入数据和选择的统计方法，它会帮我们挑出最重要的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: \n",
      "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
      "Features: \n",
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " ...\n",
      " [121.  112.   26.2  30. ]\n",
      " [126.    0.   30.1  47. ]\n",
      " [ 93.    0.   30.4  23. ]]\n"
     ]
    }
   ],
   "source": [
    "# 单变量特征选定(K方检验)(Less Recommended)\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "filename = 'pima_data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = read_csv(filename, names = names)\n",
    "array = data.values\n",
    "\n",
    "X = array[:, 0 : 8]\n",
    "Y = array[:, 8]\n",
    "\n",
    "test = SelectKBest(score_func = chi2, k = 4) # 选取评分最高的4个指标，评分越高说明指标越好\n",
    "fit = test.fit(X, Y)\n",
    "set_printoptions(precision = 3)\n",
    "# 打印评分\n",
    "print(\"Scores: \")\n",
    "print(fit.scores_)\n",
    "# 打印特征\n",
    "features = fit.transform(X)\n",
    "print(\"Features: \")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 递归特征消除\n",
    "\n",
    "### 递归特征消除（RFE）的基本概念\n",
    "\n",
    "想象你是一位雕塑家，面前有一块大石头。你的任务是雕刻出一个精美的雕塑。在这个过程中，你会一次又一次地移除石头的一部分，每一次都更接近最终的艺术作品。这就像递归特征消除的过程：从一堆特征中**逐步**移除最不重要的，直到剩下最能代表数据的那部分。\n",
    "\n",
    "在机器学习中，RFE工作如下：\n",
    "\n",
    "1. **开始训练模型**：首先，需要有一个“基模型”（比如一个决策树或逻辑回归模型），这个模型使用所有可用的特征进行训练。就像你开始时有整块石头，这时模型有所有的特征。\n",
    "\n",
    "2. **评估特征的重要性**：模型训练后，每个特征都会被赋予一个重要性分数。这就像你评估雕塑的每一部分，决定哪些部分不是最终作品的一部分。\n",
    "\n",
    "3. **移除最不重要的特征**：然后，根据这些分数，RFE会移除一些被认为最不重要的特征。这就像你从石头上切下最不需要的部分。\n",
    "\n",
    "4. **重复过程**：接下来，使用剩下的特征重新训练模型。这个过程就像是对雕塑的进一步细化。然后再次评估特征，移除最不重要的。\n",
    "\n",
    "5. **最终模型**：这个过程重复多次，直到达到预定的特征数量或者模型性能达到最优。最后，你得到一个只包含最重要特征的模型，就像最后完成的雕塑只留下最精华的部分。\n",
    "\n",
    "### RFE的好处\n",
    "\n",
    "- **简化模型**：移除不必要的特征，使模型更简洁，易于理解。\n",
    "- **提高性能**：有时候移除无关特征可以提高模型的预测性能。\n",
    "- **防止过拟合**：减少特征的数量有助于降低模型过度适应训练数据（过拟合）的风险。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features: \n",
      "3\n",
      "Selected features: \n",
      "[ True False False False False  True  True False]\n",
      "The ranking of features: \n",
      "[1 2 4 6 5 1 1 3]\n"
     ]
    }
   ],
   "source": [
    "# 递归特征消除 (LogisticRegression作为基模型) (More Recommended)\n",
    "# 以逻辑回归为基模型，通过RFE来选定对预测结果影响最大的三个数据特征。\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "filename = 'pima_data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = read_csv(filename, names = names)\n",
    "array = data.values\n",
    "\n",
    "X = array[:, 0 : 8]\n",
    "Y = array[:, 8]\n",
    "\n",
    "model = LogisticRegression(max_iter = 2000)\n",
    "rfe = RFE(model, n_features_to_select = 3) # 选择三个特征个数\n",
    "fit = rfe.fit(X, Y)\n",
    "\n",
    "print(\"The number of features: \")\n",
    "print(fit.n_features_)\n",
    "\n",
    "print(\"Selected features: \")\n",
    "print(fit.support_)\n",
    "\n",
    "print(\"The ranking of features: \")\n",
    "print(fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据降维技术\n",
    "\n",
    "主成分分析（PCA）和线性判别分析（LDA）是两种常用的数据降维技术。\n",
    "\n",
    "### 主成分分析（PCA）\n",
    "\n",
    "1. **基本概念**：\n",
    "   - PCA是一种统计方法，它通过线性变换把数据转换到一个新的坐标系统中，这个新的坐标系统的前几个坐标（主成分）能够保留数据的**最多变异性**。我们可以将其想象为一种“数据压缩”技术。\n",
    "\n",
    "2. **如何工作**：\n",
    "   - 想象一组数据点在三维空间中分布。这些点可能在某个平面或直线上更紧密地聚集。PCA找到一个新的视角，将这些点投影到一个或两个维度上，同时尽可能保留原始数据的信息（变异性）。这种投影就像从多维降到低维的“阴影”。\n",
    "\n",
    "3. **无监督学习**：\n",
    "   - 由于PCA**不考虑数据点的标签或结果**，它是一种无监督学习方法。它**只关注数据点的分布**，而不关心这些点表示什么。\n",
    "\n",
    "### 线性判别分析（LDA）\n",
    "\n",
    "1. **基本概念**：\n",
    "   - LDA不仅是一种降维技术，也是一种**分类方法**。它试图找到一个线性组合的特征，这种组合可以最佳地区分不同类别的样本。\n",
    "\n",
    "2. **如何工作**：\n",
    "   - 如果PCA是找到数据的“最佳阴影”，那么LDA就是找到最能区分不同组（或类别）的“阴影”。LDA关注的是**类别间的差异**，而不是数据的整体分布。\n",
    "\n",
    "3. **有监督学习**：\n",
    "   - LDA使用数据点的标签信息，这使它成为有监督学习的一部分。它利用这些标签来确保降维后的数据在类别间有最大的可分性。\n",
    "\n",
    "### 聚类与PCA\n",
    "\n",
    "- 在聚类算法中，使用PCA可以帮助我们在更低维度的空间中观察和解释数据，这有助于发现数据中的自然分组。降维可以减少噪声和计算复杂性，同时保留重要的分类信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解释方差:  [0.889 0.062 0.026]\n",
      "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
      "   5.372e-04 -3.565e-03]\n",
      " [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n",
      "  -8.168e-04 -1.402e-01]\n",
      " [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n",
      "  -6.400e-04 -1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "# 主成分分析 (Less recommended) (解释性异常重要，不仅要选，还要有所解释)\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "filename = 'pima_data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = read_csv(filename, names = names)\n",
    "array = data.values\n",
    "\n",
    "X = array[:, 0 : 8]\n",
    "Y = array[:, 8]\n",
    "\n",
    "pca = PCA(n_components = 3) \n",
    "fit = pca.fit(X)\n",
    "set_printoptions(precision = 3)\n",
    "\n",
    "print(\"解释方差: \", fit.explained_variance_ratio_) # 值越大越好\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征重要性\n",
    "\n",
    "特征重要性是机器学习中的一个关键概念，它帮助我们了解**哪些特征**在预测模型中起着重要的作用。袋装决策树算法、随机森林算法和极端随机树算法都是用于计算特征重要性的有效工具。\n",
    "\n",
    "### 1. 袋装决策树（Bagged Decision Trees）\n",
    "\n",
    "- **概念**：\n",
    "  - 袋装决策树是一种**集成学习方法**，它通过**构建多个决策树并“平均”它们的预测**来提高整体模型的稳定性和准确性。这类似于询问多个专家的意见而不是依赖单一专家的决定。\n",
    "\n",
    "- **特征重要性**：\n",
    "  - 在袋装模型中，特征重要性可以通过观察每个特征在决策树构建过程中的影响来计算。如果一个特征在多个树中频繁用于制定关键决策，它被认为是更重要的。\n",
    "\n",
    "### 2. 随机森林（Random Forest）\n",
    "\n",
    "- **概念**：\n",
    "  - 随机森林是袋装决策树的一个扩展。它不仅通过构建多个决策树来提高性能，而且在构建这些树时引入了**随机性**。具体来说，每个树都是在数据集的一个随机子集上训练的，并且在分裂节点时只考虑特征的一个随机子集。\n",
    "\n",
    "- **特征重要性**：\n",
    "  - 在随机森林中，特征重要性通常通过计算每个特征在树决策中的**平均不纯度减少**（例如，基尼不纯度或信息增益）来确定。**特征越能帮助降低不纯度，它就越重要。**\n",
    "\n",
    "### 3. 极端随机树（Extra Trees or Extremely Randomized Trees）\n",
    "\n",
    "- **概念**：\n",
    "  - 极端随机树与随机森林类似，但在分裂决策中引入了更多的随机性。在极端随机树中，对于每个特征，在其所有可能的分裂点中**随机选择一个**，而不是寻找最优分裂点。\n",
    "\n",
    "- **特征重要性**：\n",
    "  - 就像随机森林一样，极端随机树通过评估特征在减少树节点不纯度中的作用来计算特征重要性。更随机的分裂策略意味着这种方法对数据中的噪声和异常值不那么敏感。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The importances of feactures: \n",
      "[0.11  0.235 0.095 0.079 0.075 0.147 0.118 0.141]\n"
     ]
    }
   ],
   "source": [
    "# 极端随机树\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "filename = 'pima_data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = read_csv(filename, names = names)\n",
    "array = data.values\n",
    "\n",
    "X = array[:, 0 : 8]\n",
    "Y = array[:, 8]\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "fit = model.fit(X, Y)\n",
    "set_printoptions(precision = 3)\n",
    "\n",
    "print(\"The importances of feactures: \")\n",
    "print(fit.feature_importances_) # 选定得分高的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四种数据特征选定方法的对比\n",
    "\n",
    "| 方法 | 类型 | 监督/无监督 | 主要目的 | 应用场景 |\n",
    "|------|------|-------------|----------|--------|\n",
    "| 单变量特征选定 | 统计方法 | 取决于所用统计方法 | 选择与结果最相关的特征 | 基本特征筛选 |\n",
    "| 递归特征消除 (RFE) | 机器学习模型 | 有监督 | 逐步移除最不重要的特征 | 特征选择与优化 |\n",
    "| 主成分分析 (PCA) | 线性变换 | 无监督 | 降维同时保留最多信息 | 数据压缩与可视化 |\n",
    "| 特征的重要性 | 集成学习 | 有监督/无监督 | 评估特征在模型预测中的重要性 | 评估与优化模型特征 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
