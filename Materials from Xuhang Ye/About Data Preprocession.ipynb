{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "\n",
    "## 为什么需要数据预处理\n",
    "\n",
    "1. **确保数据的质量**：在真实世界中，数据往往是**不完整（缺少某些感兴趣的属性值）**、**不一致（包含代码或者名称的差异）**并且**充满噪声（错误或异常值）**。\n",
    "\n",
    "2. **增强模型的性能**：高质量的数据可以显著提高分析模型的准确度和效率。\n",
    "\n",
    "3. **提高结果的可解释性**：干净且一致的数据使得分析结果更易于理解和解释。\n",
    "\n",
    "## 数据预处理的流程\n",
    "\n",
    "1. **导入数据**：首先，我们需要从各种来源导入数据。\n",
    "\n",
    "2. **整理数据**：\n",
    "   - **处理缺失值**：识别并处理数据中的缺失值。\n",
    "   - **识别和处理异常值**：对于数据中的不良部分，需要被剔除或修正。\n",
    "   - **数据清洗**：移除重复数据，纠正错误，确保数据干净可用。\n",
    "\n",
    "3. **格式化输入数据**：\n",
    "   - **数据转换**：根据需要将数据转换成适当的格式或结构。\n",
    "   - **标准化或归一化数据**：确保数据在同一尺度上，以便于分析。\n",
    "\n",
    "4. **总结数据**：\n",
    "    - 涉及到对数据的整体了解，包括它的分布、趋势和关系。\n",
    "\n",
    "## 数据预处理的步骤\n",
    "\n",
    "### a. 数据标准化和变换\n",
    "- **scikit-learn 的数据预处理**：scikit-learn 提供多种数据预处理功能。两个主要的方法是 `fit` 和 `transform`。\n",
    "  - **`fit()` 方法**：计算用于后续转换的必要参数。比如，在标准化过程中，`fit()` 将计算数据的平均值和标准差。\n",
    "  - **`transform()` 方法**：使用 `fit()` 计算出的参数来转换数据。转换后的数据将具有一致的结构，有助于模型训练。\n",
    "\n",
    "- **适合和变换的多重变化**：在实际应用中，我们可能需要对数据集进行一系列的变换。例如，我们可能需要先标准化数据，然后应用某种降维技术。scikit-learn 通过管道（pipeline）机制允许我们方便地将这些步骤链接起来。\n",
    "\n",
    "### b. 调整数据尺度\n",
    "- **为什么调整尺度**：不同特征可能存在不同的量度（如一些以千为单位，另一些以百分比计），这会影响某些算法的性能，特别是那些基于距离的算法，如K-近邻、神经网络等。\n",
    "\n",
    "- **标准化方法**：一种常见的标准化方法是将特征缩放到 0 到 1 的范围内。这就是所谓的 Min-Max 缩放。scikit-learn 提供了 `MinMaxScaler` 来实现这个过程。\n",
    "\n",
    "- **统计学中的尺度概念**：在统计学中，尺度被分为定类尺度、定序尺度、定距尺度和定比尺度。这些尺度涉及到数据的测量和解释方式，其中定比尺度是最高级的尺度，提供最多的信息（如绝对零点和等比距离）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.353 0.744 0.59  ... 0.501 0.234 0.483]\n",
      " [0.059 0.427 0.541 ... 0.396 0.117 0.167]\n",
      " [0.471 0.92  0.525 ... 0.347 0.254 0.183]\n",
      " ...\n",
      " [0.294 0.608 0.59  ... 0.39  0.071 0.15 ]\n",
      " [0.059 0.633 0.492 ... 0.449 0.116 0.433]\n",
      " [0.059 0.467 0.574 ... 0.453 0.101 0.033]]\n"
     ]
    }
   ],
   "source": [
    "# 归一化，使用MinMaxScaler(按比例缩放，less recommended)\n",
    "# K近邻算法\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import set_printoptions\n",
    "from pandas import read_csv\n",
    "\n",
    "filename = 'pima_data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = read_csv(filename, names = names)\n",
    "\n",
    "array = data.values\n",
    "X = array[ : , 0 : 8]\n",
    "Y = array[ : , 8]\n",
    "transformer = MinMaxScaler(feature_range = (0, 1))\n",
    "newX = transformer.fit_transform(X)\n",
    "set_printoptions(precision = 3)\n",
    "print(newX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64   0.848  0.15  ...  0.204  0.468  1.426]\n",
      " [-0.845 -1.123 -0.161 ... -0.684 -0.365 -0.191]\n",
      " [ 1.234  1.944 -0.264 ... -1.103  0.604 -0.106]\n",
      " ...\n",
      " [ 0.343  0.003  0.15  ... -0.735 -0.685 -0.276]\n",
      " [-0.845  0.16  -0.471 ... -0.24  -0.371  1.171]\n",
      " [-0.845 -0.873  0.046 ... -0.202 -0.474 -0.871]]\n"
     ]
    }
   ],
   "source": [
    "# 正态化，使用StandardScaler\n",
    "# 线性回归、逻辑回归、判别分析\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "transformer = StandardScaler().fit(X)\n",
    "newX = transformer.transform(X)\n",
    "\n",
    "print(newX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.034 0.828 0.403 ... 0.188 0.004 0.28 ]\n",
      " [0.008 0.716 0.556 ... 0.224 0.003 0.261]\n",
      " [0.04  0.924 0.323 ... 0.118 0.003 0.162]\n",
      " ...\n",
      " [0.027 0.651 0.388 ... 0.141 0.001 0.161]\n",
      " [0.007 0.838 0.399 ... 0.2   0.002 0.313]\n",
      " [0.008 0.736 0.554 ... 0.241 0.002 0.182]]\n"
     ]
    }
   ],
   "source": [
    "# 标准化，使用Normalizer(通过计算，more recommended)\n",
    "# 神经网络、K近邻算法\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "transformer = Normalizer().fit(X)\n",
    "newX = transformer.transform(X)\n",
    "\n",
    "print(newX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 二值数据，使用Binarizer(大于某阈值为1，小于某阈值为0。最不常用)\n",
    "# 分类分级\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "transformer = Binarizer(threshold = 0.0).fit(X)\n",
    "newX = transformer.transform(X)\n",
    "\n",
    "print(newX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么需要处理数据特征\n",
    "\n",
    "1. **数据的重要性**：数据和特征决定了机器学习的上限，而模型和算法仅仅是尝试接近这个上限。好的特征能够显著提升模型性能。\n",
    "\n",
    "2. **特征工程的目的**：特征工程的核心是从原始数据中提取最合适的特征，供算法和模型使用。通过选择和转换输入的数据，可以增强模型的预测能力。\n",
    "\n",
    "3. **算法性能提升**：合适的特征选定可以减少模型复杂度，提高算法的准确性，并减少训练时间。\n",
    "\n",
    "## 特征选定的方法\n",
    "\n",
    "Scikit-learn 提供了多种特征处理方法，包括但不限于：\n",
    "\n",
    "1. **单变量特征选定**：这种方法考察单个变量与目标变量之间的关系。基于某种统计测试（如卡方测试、ANOVA），它选择与输出最相关的特征。\n",
    "\n",
    "2. **递归特征消除（Recursive Feature Elimination, RFE）**：这种方法通过递归减少特征的数量。它利用模型的权重来识别并去除最不重要的特征，逐步优化特征子集。\n",
    "\n",
    "3. **主成分分析（PCA）**：PCA 是一种降维技术，通过线性变换将数据转换为一组相互正交的变量（主成分），这些主成分捕捉了数据中的大部分变异。\n",
    "\n",
    "4. **特征的重要性**：一些模型（如决策树和随机森林）可以提供关于特征重要性的内置方法，帮助识别对模型预测最有贡献的特征。\n",
    "\n",
    "## 特征选定的优点\n",
    "\n",
    "1. **降低拟合度（Overfitting）**：去除不相关的特征可以减少模型过度拟合数据的风险，提高其泛化能力。\n",
    "\n",
    "2. **提高算法精度**：减少噪声数据，专注于对结果影响最大的特征，可以显著提高算法的预测准确度。\n",
    "\n",
    "3. **减少训练时间**：特征较少的数据集通常意味着更快的训练速度和更低的计算成本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
